# -*- coding: utf-8 -*-
"""ae_color_v3.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11eBSz3FHYxRLxHblDhWT_UjLJtItlY7b
"""

# from google.colab import drive
# drive.mount('/content/drive', force_remount=False)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# !rm -rf ./logs/

import os, sys, glob, re, time
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, losses
from tensorflow.keras.models import Model

from keras.layers import Conv2D, MaxPooling2D, Dense, Input, UpSampling2D, BatchNormalization
from keras.layers import merge, concatenate
from keras.callbacks import EarlyStopping, TensorBoard
from keras.layers import GaussianNoise


def numerical_sort(value):
    """
    Splits out any digits in a filename, turns it into an actual number, and returns the result for sorting.
    :param value: filename
    :return:

    author: kei
    date: 20190903
    """
    numbers = re.compile(r'(\d+)')
    parts = numbers.split(value)
    parts[1::2] = map(int, parts[1::2])
    return parts


def visualize_ds(images, max_samples=20):
    samples = min(len(images), max_samples)

    fig = plt.figure(figsize=(10,samples))
    fig.subplots_adjust(hspace=0.1)
  
    for p in range(samples):
        ax = fig.add_subplot(samples//4, 4, p+1)
        ax.axis('off')
        ax.imshow(images[p])
    

def process_ds(paths, vis=False):
    print('total images', paths)
    for p in paths:
        img = plt.imread(p)
        img = cv2.resize(img, (height, width))
        BATCH.append(img) #global variable

    if vis:
        visualize_ds(BATCH)


# clone sample images
# path = '/content/drive/MyDrive/ds/pushing'
path = '/home/ryo/Dataset/dataset'
sys.path.append(path)
#dirs = os.listdir(path)
dirs = ['pushing']

height = 160
width = 80
BATCH = []

def load_dataset(visualize=False):
    start = time.time()
    for dir in dirs:
        images = sorted(glob.glob(os.path.join(path, dir, 'group*', 'image_frame*.jpg'), recursive=True), key=numerical_sort)

        #images = sorted(glob.glob(os.path.join(path, dir, 'group45', 'image_frame*.jpg'), recursive=True), key=numerical_sort)
        images = images[::5]
        process_ds(images, vis=visualize)

    end = time.time()
    print('total time spent {}'.format((end-start)/60))


load_dataset()
print(len(BATCH))

start = time.time()

ds = tf.stack(BATCH) #create tensor of samples

end = time.time()
print('total time spent {}'.format((end-start)/60))

ratio = int(len(ds)*.7)

train_ds = ds[1:ratio,:]
test_ds = ds[ratio:,:]

print(train_ds.shape, test_ds.shape)

train_ds = train_ds / 255
test_ds = test_ds / 255
gaussian_train_ds = train_ds
gaussian_test_ds = test_ds

#plt.imshow((gaussian_train_ds[1,:]*255).astype(np.uint8))

# https://www.tensorflow.org/tutorials/generative/autoencoder
# put the Activation layer AFTER the BatchNormalization() layer

# initialize the model
dense_dim = 64
latent_dim = 64
channels_ = 64
dropout = 0.25

def model():
    encoder_input = tf.keras.Input(shape=(width, height, 3))
    x = GaussianNoise(0.2)(encoder_input)
    x = tf.keras.layers.Conv2D(8, kernel_size=3, strides=1, padding='same', activation='selu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    c1 = tf.keras.layers.MaxPool2D(pool_size=2)(x)

    x = tf.keras.layers.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='selu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    c2 = tf.keras.layers.MaxPool2D(pool_size=2)(x)
    
    x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='selu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    c3 = tf.keras.layers.MaxPool2D(pool_size=2)(x)
    
    x = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', activation='selu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    c4 = tf.keras.layers.MaxPool2D(pool_size=2)(x)

    x = tf.keras.layers.Flatten()(c4)
    d4 = tf.keras.layers.Dense(dense_dim, activation='selu')(x)

    x = tf.keras.layers.Flatten()(c3)
    d3 = tf.keras.layers.Dense(dense_dim, activation='selu')(x)

    x = tf.keras.layers.Flatten()(c2)
    d2 = tf.keras.layers.Dense(dense_dim, activation='selu')(x)

    x = tf.keras.layers.concatenate([d4, d3, d2])
    encoder_output = tf.keras.layers.Dense(latent_dim, activation='selu')(x)
    encoder = tf.keras.Model(encoder_input, encoder_output, name='encoder')
    encoder.summary()

    decoder_input = tf.keras.Input(shape=(latent_dim))
    dense_vector = tf.keras.layers.Dense(dense_dim*3, activation='selu')(decoder_input)
    dd4 = tf.keras.layers.Lambda(lambda x: x[:,:64], output_shape=(64))(dense_vector)
    dd3 = tf.keras.layers.Lambda(lambda x: x[:,64:128], output_shape=(64))(dense_vector)
    dd2 = tf.keras.layers.Lambda(lambda x: x[:,128:192], output_shape=(64))(dense_vector)

    x = tf.keras.layers.Dense(5*10*64, activation='selu')(dd4)
    dc4 = tf.keras.layers.Reshape(target_shape=(5, 10, 64))(x)

    x = tf.keras.layers.Dense(10*20*32, activation='selu')(dd3)
    dc3 = tf.keras.layers.Reshape(target_shape=(10, 20, 32))(x)

    x = tf.keras.layers.Dense(20*40*16, activation='selu')(dd2)
    dc2 = tf.keras.layers.Reshape(target_shape=(20, 40, 16))(x)

    x = tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding='same', activation='selu')(dc4) # 10,20,32
    x = tf.keras.layers.concatenate([x, dc3], axis=3) # 10,20,64
    x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='selu')(x)
    x = tf.keras.layers.BatchNormalization()(x) # 10,20,32
    # DoubleConv?
    # x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='selu')(x)    
    # x = tf.keras.layers.BatchNormalization()(x) # 10,20,32
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding='same', activation='selu')(x) # 20,40,16
    x = tf.keras.layers.concatenate([x, dc2], axis=3) # 20,40,32
    x = tf.keras.layers.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='selu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    
    x = tf.keras.layers.Conv2DTranspose(8, kernel_size=3, strides=2, padding='same', activation='selu')(x) # 40,80,8
          
    decoder_output = tf.keras.layers.Conv2DTranspose(3, kernel_size=3, strides=2, padding='same', activation='sigmoid')(x) # 80,160,8
    decoder = tf.keras.Model(decoder_input, decoder_output, name='decoder')
    decoder.summary()

    return encoder, decoder

#opt = tf.keras.optimizers.Adam(learning_rate=0.002)
opt = keras.optimizers.Adamax(learning_rate=0.001)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    encoder, decoder = model()
    autoencoder_input = tf.keras.Input(shape=(width, height, 3), name='img')
    decoded_img = decoder(encoder(autoencoder_input))
    auto_encoder = tf.keras.Model(autoencoder_input, decoded_img, name='autoencoder')
    auto_encoder.compile(loss='mse', optimizer=opt)

# create checkpoint and save best weight
checkpoint_path = "/home/ryo/Program/Ashesh_colab/ae_cp/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)


def train():

    start = time.time()

    # Create a callback that saves the model's weights
    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                     save_weights_only=True,
                                                     verbose=1,
                                                     mode='min',
                                                     save_best_only=True)

    # early stopping if not changing for 50 epochs
    early_stop = EarlyStopping(monitor='val_loss',
                               patience=100)

    # reduce learning rate
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', 
                                                     factor=0.1,
                                                     patience=50, 
                                                     verbose=1,
                                                     min_lr=0.00001)

    # train the model
    gaussian_history = auto_encoder.fit(gaussian_train_ds,
                                        train_ds, 
                                        epochs=500,
                                        batch_size=64,
                                        shuffle=True,
                                        validation_data=(gaussian_test_ds, test_ds),
                                        callbacks=[cp_callback, early_stop, reduce_lr])

    end = time.time()
    print('\ntotal time spent {}'.format((end-start)/60))

    plt.plot(gaussian_history.epoch, gaussian_history.history['loss'], label='train_loss')
    plt.plot(gaussian_history.epoch, gaussian_history.history['val_loss'], label='test_loss')
    plt.title('Epochs on Training Loss')
    plt.xlabel('# of Epochs')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.show()
    return gaussian_history


def test():
    # load_best_checkpoint and evaluate
    cp_model = auto_encoder
    cp_model.compile(loss='mse', optimizer=opt)
    cp_model.load_weights(checkpoint_path)
    cp_loss = cp_model.evaluate(gaussian_test_ds, test_ds)

    # evaluate the model on the test set
    # final_loss = gaussian_auto_encoder.evaluate(gaussian_test_ds, test_ds)

    """# DENOISED IMAGES"""

    # run model on the test_ds to reconstruct
    cp_result = cp_model.predict(gaussian_test_ds)
    
    n = 10
    #idx = [np.random.randint(1,20) for i in range(n)]
    idx = [np.random.randint(1,20) for i in range(n)]
    plt.figure(figsize=(20, 4))
    for i in range(n):
        # display original
        ax = plt.subplot(3, n, i + 1)
        plt.title("original")
        plt.imshow(test_ds[idx[i]])
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

        # display reconstruction
        cx = plt.subplot(3, n, i + n + 1)
        plt.title("reconstructed")
        plt.imshow(cp_result[idx[i]])
        cx.get_xaxis().set_visible(False)
        cx.get_yaxis().set_visible(False)

    plt.show()

    # final_result = gaussian_auto_encoder.predict(gaussian_test_ds)

    # samples = len(final_result)
    # fig = plt.figure(figsize=(15, samples))
    # for p in range(1, samples):
    #     ax = fig.add_subplot(samples//2, 5, p+1)
    #     ax.imshow((final_result[p]*255).astype(np.uint8))
    #     ax.axis('off')

    # """# Original Test Images"""
    
    # fig = plt.figure(figsize=(15, samples))
    # for p in range(1, samples):
    #     ax = fig.add_subplot(samples//2, 5, p+1)
    #     ax.imshow(test_ds[p])
    #     ax.axis('off')
